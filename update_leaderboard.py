import requests
import json
import time
import logging
import os
from datetime import datetime, timedelta, timezone # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–∞—Ç–∞–º–∏
logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s: %(message)s")
API_KEY = os.getenv("API_KEY")
COMMUNITY_ID = "1951903018464772103"
BASE_URL = f"https://api.socialdata.tools/twitter/community/{COMMUNITY_ID}/tweets"
HEADERS = {"Authorization": f"Bearer {API_KEY}"}
TWEETS_FILE = "all_tweets.json"
LEADERBOARD_FILE = "leaderboard.json"
def is_within_last_n_days(created_at_str, days=60):
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –±—ã–ª–∞ –ª–∏ –¥–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ç–≤–∏—Ç–∞ (–≤ —Ñ–æ—Ä–º–∞—Ç–µ ISO 8601) –≤ —Ç–µ—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö N –¥–Ω–µ–π.
    """
    # API –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–∞—Ç—É –≤ —Ñ–æ—Ä–º–∞—Ç–µ ISO 8601, –Ω–∞–ø—Ä–∏–º–µ—Ä: "2025-04-01T12:34:56.000Z"
    try:
        tweet_time = datetime.fromisoformat(created_at_str.replace("Z", "+00:00"))
    except ValueError:
        # –ï—Å–ª–∏ —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç—ã –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π, —Å—á–∏—Ç–∞–µ–º, —á—Ç–æ —Ç–≤–∏—Ç "—Å—Ç–∞—Ä—ã–π"
        logging.warning(f"–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç—ã: {created_at_str}")
        return False
    now = datetime.now(timezone.utc)
    n_days_ago = now - timedelta(days=days)
    # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º timestamp
    return tweet_time >= n_days_ago
def load_json(path):
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        return []
def save_json(path, data):
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
def fetch_tweets(cursor=None, limit=50):
    params = {"type": "Latest", "limit": limit}
    if cursor:
        params["cursor"] = cursor
    r = requests.get(BASE_URL, headers=HEADERS, params=params)
    r.raise_for_status()
    return r.json()
def collect_all_tweets():
    all_tweets = []
    seen_ids = set()
    cursor = None
    total_new = 0
    while True:
        data = fetch_tweets(cursor)
        tweets = data.get("tweets", [])
        cursor = data.get("next_cursor")
        if not tweets:
            break
        # --- –ò–ó–ú–ï–ù–ï–ù–ò–ï: –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –¥–∞—Ç–µ (—Ç–µ–ø–µ—Ä—å –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 60 –¥–Ω–µ–π) ---
        new_tweets = [t for t in tweets if t["id_str"] not in seen_ids and is_within_last_n_days(t.get("created_at"), days=60)]
        # --- –ö–û–ù–ï–¶ –ò–ó–ú–ï–ù–ï–ù–ò–Ø ---
        if not new_tweets:
            logging.info("–î–æ—Å—Ç–∏–≥–Ω—É—Ç—ã —Ç–≤–∏—Ç—ã –∑–∞ –ø—Ä–µ–¥–µ–ª–∞–º–∏ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 60 –¥–Ω–µ–π, –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–±–æ—Ä–∞.")
            break
        all_tweets.extend(new_tweets)
        seen_ids.update(t["id_str"] for t in new_tweets)
        total_new += len(new_tweets)
        logging.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(new_tweets)} –Ω–æ–≤—ã—Ö —Ç–≤–∏—Ç–æ–≤ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 60 –¥–Ω–µ–π (–≤—Å–µ–≥–æ: {len(all_tweets)})")
        if not cursor:
            break
        time.sleep(3)
    # --- –ò–ó–ú–ï–ù–ï–ù–ò–ï: –°–æ—Ö—Ä–∞–Ω—è–µ–º –¢–û–õ–¨–ö–û —Ç–≤–∏—Ç—ã –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 60 –¥–Ω–µ–π ---
    save_json(TWEETS_FILE, all_tweets)
    # --- –ö–û–ù–ï–¶ –ò–ó–ú–ï–ù–ï–ù–ò–Ø ---
    logging.info(f"–°–±–æ—Ä –∑–∞–≤–µ—Ä—à—ë–Ω. –í—Å–µ–≥–æ —Ç–≤–∏—Ç–æ–≤ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 60 –¥–Ω–µ–π: {len(all_tweets)}") # <-- –ò–°–ü–†–ê–í–õ–ï–ù–û
    return all_tweets
def build_leaderboard(tweets):
    leaderboard = {}
    for t in tweets: # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ —Ç–≤–∏—Ç—ã, –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–µ –≤ —Ñ—É–Ω–∫—Ü–∏—é (—Ç.–µ. –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 60 –¥–Ω–µ–π)
        user = t.get("user")
        if not user:
            continue
        name = user.get("screen_name")
        if not name:
            continue
        stats = leaderboard.setdefault(name, {
            "posts": 0,
            "likes": 0,
            "retweets": 0,
            "comments": 0,
            "quotes": 0,
            "views": 0
        })
        stats["posts"] += 1
        stats["likes"] += t.get("favorite_count", 0)
        stats["retweets"] += t.get("retweet_count", 0)
        stats["comments"] += t.get("reply_count", 0)
        stats["quotes"] += t.get("quote_count", 0)
        stats["views"] += t.get("views_count", 0)
    leaderboard_list = [[user, stats] for user, stats in leaderboard.items()]
    save_json(LEADERBOARD_FILE, leaderboard_list)
    logging.info(f"üèÜ –õ–∏–¥–µ—Ä–±–æ—Ä–¥ –æ–±–Ω–æ–≤–ª—ë–Ω ({len(leaderboard_list)} —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 60 –¥–Ω–µ–π).")
# --- –ù–û–í–´–ô –ë–õ–û–ö: –°–û–ó–î–ê–ù–ò–ï –î–ê–ù–ù–´–• –î–õ–Ø –ì–†–ê–§–ò–ö–ê ---
def build_daily_stats(tweets):
    """
    –°–æ–±–∏—Ä–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –¥–Ω—è–º: —Å–∫–æ–ª—å–∫–æ –ø–æ—Å—Ç–æ–≤ –±—ã–ª–æ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ –≤ –∫–∞–∂–¥—ã–π –¥–µ–Ω—å (–∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 60 –¥–Ω–µ–π).
    """
    daily_stats = {}
    for t in tweets:
        created_at_str = t.get("created_at")
        if not created_at_str:
            continue
        # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—É
        try:
            tweet_date = datetime.fromisoformat(created_at_str.replace("Z", "+00:00")).date()
        except ValueError:
            logging.warning(f"–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç—ã: {created_at_str}")
            continue
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á—ë—Ç—á–∏–∫ –¥–ª—è —ç—Ç–æ–≥–æ –¥–Ω—è
        daily_stats[tweet_date] = daily_stats.get(tweet_date, 0) + 1
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å–ø–∏—Å–æ–∫ –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ (–¥–∞—Ç–∞ -> –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ)
    daily_list = [{"date": str(date), "posts": count} for date, count in sorted(daily_stats.items())]
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª
    save_json("daily_posts.json", daily_list)
    logging.info(f"üìä –ì—Ä–∞—Ñ–∏–∫ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –æ–±–Ω–æ–≤–ª—ë–Ω ({len(daily_list)} –¥–Ω–µ–π –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 60 –¥–Ω–µ–π).")
if __name__ == "__main__":
    tweets = collect_all_tweets()
    build_leaderboard(tweets)
    build_daily_stats(tweets)  # –ó–∞–ø—É—Å–∫–∞–µ–º –Ω–æ–≤—É—é —Ñ—É–Ω–∫—Ü–∏—é
