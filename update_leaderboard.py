import requests
import json
import time
import logging
import os
from datetime import datetime, timedelta, timezone # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–∞—Ç–∞–º–∏

logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s: %(message)s")

API_KEY = os.getenv("API_KEY")
COMMUNITY_ID = "1951903018464772103"
BASE_URL = f"https://api.socialdata.tools/twitter/community/{COMMUNITY_ID}/tweets"
HEADERS = {"Authorization": f"Bearer {API_KEY}"}

TWEETS_FILE = "all_tweets.json"
LEADERBOARD_FILE = "leaderboard.json"

# –£–î–ê–õ–Ø–ï–ú —Ñ—É–Ω–∫—Ü–∏—é is_within_last_n_days, –æ–Ω–∞ –±–æ–ª—å—à–µ –Ω–µ –Ω—É–∂–Ω–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø—Ä–∏ —Å–±–æ—Ä–µ

def load_json(path):
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        return []

def save_json(path, data):
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def fetch_tweets(cursor=None, limit=50):
    params = {"type": "Latest", "limit": limit}
    if cursor:
        params["cursor"] = cursor
    r = requests.get(BASE_URL, headers=HEADERS, params=params)
    r.raise_for_status()
    return r.json()

def collect_all_tweets():
    all_tweets = []
    seen_ids = set()
    cursor = None
    total_new = 0
    while True:
        data = fetch_tweets(cursor)
        tweets = data.get("tweets", [])
        cursor = data.get("next_cursor")
        if not tweets:
            break
        # --- –ò–ó–ú–ï–ù–ï–ù–ò–ï: –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ ID (–¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –≤ —Ä–∞–º–∫–∞—Ö –æ–¥–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞) ---
        # –£–±–∏—Ä–∞–µ–º —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –ø–æ –¥–∞—Ç–µ –ø—Ä–∏ —Å–±–æ—Ä–µ. –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ "Latest", –Ω–∞—Å–∫–æ–ª—å–∫–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç API.
        new_tweets = [t for t in tweets if t["id_str"] not in seen_ids]
        # --- –ö–û–ù–ï–¶ –ò–ó–ú–ï–ù–ï–ù–ò–Ø ---
        if not new_tweets:
            logging.info("–ù–µ—Ç –Ω–æ–≤—ã—Ö —Ç–≤–∏—Ç–æ–≤ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è, –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–±–æ—Ä–∞.")
            break
        all_tweets.extend(new_tweets)
        seen_ids.update(t["id_str"] for t in new_tweets)
        total_new += len(new_tweets)
        logging.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(new_tweets)} –Ω–æ–≤—ã—Ö —Ç–≤–∏—Ç–æ–≤ (–≤—Å–µ–≥–æ: {len(all_tweets)})")
        if not cursor:
            break
        time.sleep(3)

    # --- –ò–ó–ú–ï–ù–ï–ù–ò–ï: –°–æ—Ö—Ä–∞–Ω—è–µ–º –í–°–ï —Å–æ–±—Ä–∞–Ω–Ω—ã–µ —Ç–≤–∏—Ç—ã ---
    save_json(TWEETS_FILE, all_tweets)
    # --- –ö–û–ù–ï–¶ –ò–ó–ú–ï–ù–ï–ù–ò–Ø ---
    logging.info(f"–°–±–æ—Ä –∑–∞–≤–µ—Ä—à—ë–Ω. –í—Å–µ–≥–æ —Ç–≤–∏—Ç–æ–≤: {len(all_tweets)}")
    return all_tweets

def build_leaderboard(tweets):
    leaderboard = {}
    for t in tweets: # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ —Ç–≤–∏—Ç—ã, –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–µ –≤ —Ñ—É–Ω–∫—Ü–∏—é
        user = t.get("user")
        if not user:
            continue
        name = user.get("screen_name")
        if not name:
            continue
        stats = leaderboard.setdefault(name, {
            "posts": 0,
            "likes": 0,
            "retweets": 0,
            "comments": 0,
            "quotes": 0,
            "views": 0
        })
        stats["posts"] += 1
        stats["likes"] += t.get("favorite_count", 0)
        stats["retweets"] += t.get("retweet_count", 0)
        stats["comments"] += t.get("reply_count", 0)
        stats["quotes"] += t.get("quote_count", 0)
        stats["views"] += t.get("views_count", 0)
    leaderboard_list = [[user, stats] for user, stats in leaderboard.items()]
    save_json(LEADERBOARD_FILE, leaderboard_list)
    logging.info(f"üèÜ –õ–∏–¥–µ—Ä–±–æ—Ä–¥ –æ–±–Ω–æ–≤–ª—ë–Ω ({len(leaderboard_list)} —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤).")

# --- –ù–û–í–´–ô –ë–õ–û–ö: –°–û–ó–î–ê–ù–ò–ï –î–ê–ù–ù–´–• –î–õ–Ø –ì–†–ê–§–ò–ö–ê ---
def build_daily_stats(tweets):
    """
    –°–æ–±–∏—Ä–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –¥–Ω—è–º: —Å–∫–æ–ª—å–∫–æ –ø–æ—Å—Ç–æ–≤ –±—ã–ª–æ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ –≤ –∫–∞–∂–¥—ã–π –¥–µ–Ω—å
    (–Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Å–µ—Ö —Ç–≤–∏—Ç–æ–≤, —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö –≤ —Ç–µ–∫—É—â–µ–º –∑–∞–ø—É—Å–∫–µ).
    """
    daily_stats = {}
    for t in tweets:
        created_at_str = t.get("created_at")
        if not created_at_str:
            continue
        # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—É
        try:
            tweet_date = datetime.fromisoformat(created_at_str.replace("Z", "+00:00")).date()
        except ValueError:
            logging.warning(f"–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç—ã: {created_at_str}")
            continue
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á—ë—Ç—á–∏–∫ –¥–ª—è —ç—Ç–æ–≥–æ –¥–Ω—è
        daily_stats[tweet_date] = daily_stats.get(tweet_date, 0) + 1
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å–ø–∏—Å–æ–∫ –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ (–¥–∞—Ç–∞ -> –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ)
    daily_list = [{"date": str(date), "posts": count} for date, count in sorted(daily_stats.items())]
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª
    save_json("daily_posts.json", daily_list)
    logging.info(f"üìä –ì—Ä–∞—Ñ–∏–∫ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –æ–±–Ω–æ–≤–ª—ë–Ω ({len(daily_list)} –¥–Ω–µ–π).")

if __name__ == "__main__":
    tweets = collect_all_tweets()
    build_leaderboard(tweets)
    build_daily_stats(tweets)  # –ó–∞–ø—É—Å–∫–∞–µ–º –Ω–æ–≤—É—é —Ñ—É–Ω–∫—Ü–∏—é
