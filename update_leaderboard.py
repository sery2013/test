import requests
import json
import time
import logging
import os
from datetime import datetime

logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s: %(message)s")

API_KEY = os.getenv("API_KEY")
COMMUNITY_ID = "1896991026272723220"
BASE_URL = f"https://api.socialdata.tools/twitter/community/{COMMUNITY_ID}/tweets"
HEADERS = {"Authorization": f"Bearer {API_KEY}"}

TWEETS_FILE = "all_tweets.json"
LEADERBOARD_FILE = "leaderboard.json"
LAST_UPDATED_FILE = "last_updated.txt"

def save_json(path, data):
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def save_text(path, text):
    with open(path, "w", encoding="utf-8") as f:
        f.write(text)

def fetch_tweets(cursor=None, limit=50):
    params = {"type": "Latest", "limit": limit}
    if cursor:
        params["cursor"] = cursor
    r = requests.get(BASE_URL, headers=HEADERS, params=params)
    r.raise_for_status()
    return r.json()


def collect_all_tweets():
    all_tweets = []  # –ù–∞—á–∏–Ω–∞–µ–º —Å –ø—É—Å—Ç–æ–≥–æ —Å–ø–∏—Å–∫–∞
    seen_ids = set() # –ò –ø—É—Å—Ç–æ–≥–æ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ ID
    cursor = None
    total_new = 0

    while True:
        data = fetch_tweets(cursor)
        tweets = data.get("tweets", [])
        cursor = data.get("next_cursor")

        if not tweets:
            logging.info("‚ùå –ù–µ—Ç –Ω–æ–≤—ã—Ö —Ç–≤–∏—Ç–æ–≤ –æ—Ç API.")
            break

        # –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–æ–≤—ã–µ —Ç–≤–∏—Ç—ã, –∫–æ—Ç–æ—Ä—ã—Ö –µ—â—ë –Ω–µ—Ç –≤ seen_ids –∑–∞ –≠–¢–û–¢ –∑–∞–ø—É—Å–∫
        new_tweets = [t for t in tweets if t["id_str"] not in seen_ids]

        if not new_tweets:
            logging.info("‚úÖ –ù–æ–≤—ã—Ö —Ç–≤–∏—Ç–æ–≤ –±–æ–ª—å—à–µ –Ω–µ—Ç (–≤—Å–µ –≤ –ø–∞–∫–µ—Ç–µ —É–∂–µ –≤–∏–¥–µ–ª–∏ –≤ —ç—Ç–æ–º –∑–∞–ø—É—Å–∫–µ). –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–±–æ—Ä.")
            break

        all_tweets.extend(new_tweets)
        seen_ids.update(t["id_str"] for t in new_tweets)
        total_new += len(new_tweets)

        logging.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(new_tweets)} –Ω–æ–≤—ã—Ö —Ç–≤–∏—Ç–æ–≤ (–≤—Å–µ–≥–æ –≤ —ç—Ç–æ–º –∑–∞–ø—É—Å–∫–µ: {len(all_tweets)})")

        if not cursor:
            logging.info("‚úÖ –î–æ—Å—Ç–∏–≥–Ω—É—Ç –∫–æ–Ω–µ—Ü —Å–ø–∏—Å–∫–∞ —Ç–≤–∏—Ç–æ–≤ –æ—Ç API.")
            break

        time.sleep(3) # –£–≤–∞–∂–∞–µ–º –ª–∏–º–∏—Ç—ã API

    # –ü–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º all_tweets.json –¢–û–õ–¨–ö–û –Ω–æ–≤—ã–º–∏ —Ç–≤–∏—Ç–∞–º–∏ –∑–∞ —ç—Ç–æ—Ç –∑–∞–ø—É—Å–∫
    save_json(TWEETS_FILE, all_tweets)
    logging.info(f"\n‚úÖ –°–±–æ—Ä –∑–∞–≤–µ—Ä—à—ë–Ω. –í—Å–µ–≥–æ —Ç–≤–∏—Ç–æ–≤ –≤ —Ñ–∞–π–ª–µ: {len(all_tweets)}, –Ω–æ–≤—ã—Ö: {total_new}")
    return all_tweets


def build_leaderboard(tweets):
    leaderboard = {}

    for t in tweets:
        user = t.get("user")
        if not user:
            continue
        name = user.get("screen_name")
        if not name:
            continue

        stats = leaderboard.setdefault(name, {
            "posts": 0,
            "likes": 0,
            "retweets": 0,
            "comments": 0,
            "quotes": 0,
            "views": 0
        })

        stats["posts"] += 1
        stats["likes"] += t.get("favorite_count", 0)
        stats["retweets"] += t.get("retweet_count", 0)
        stats["comments"] += t.get("reply_count", 0)
        stats["quotes"] += t.get("quote_count", 0)
        stats["views"] += t.get("views_count", 0)


    leaderboard_list = [[user, stats] for user, stats in leaderboard.items()]
    save_json(LEADERBOARD_FILE, leaderboard_list)

    # --- –ù–û–í–´–ô –ö–û–î (–∞–≤—Ç–æ–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–∞—Ç—ã) ---
    updated_at = datetime.now().strftime("%B %d, %Y")  # –ù–∞–ø—Ä–∏–º–µ—Ä: November 18, 2025
    save_text(LAST_UPDATED_FILE, updated_at)
    # -----------------

    logging.info(f"üèÜ –õ–∏–¥–µ—Ä–±–æ—Ä–¥ –æ–±–Ω–æ–≤–ª—ë–Ω ({len(leaderboard_list)} —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤).")


if __name__ == "__main__":
    tweets = collect_all_tweets()
    build_leaderboard(tweets)
