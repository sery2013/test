import requests
import json
import time
import logging
import os

logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s: %(message)s")

API_KEY = os.getenv("API_KEY")
COMMUNITY_ID = "1902883093062574425"
BASE_URL = f"https://api.socialdata.tools/twitter/community/{COMMUNITY_ID}/tweets"
HEADERS = {"Authorization": f"Bearer {API_KEY}"}

TWEETS_FILE = "all_tweets.json"
LEADERBOARD_FILE = "leaderboard.json"
# LAST_UPDATED_FILE = "last_updated.txt"  # <-- –£–î–ê–õ–ï–ù–û
KNOWN_IDS_FILE = "known_tweet_ids.txt" # <-- –ù–û–í–´–ô –§–ê–ô–õ

def save_json(path, data):
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

# def save_text(path, text):  # <-- –£–î–ê–õ–ï–ù–ê –§–£–ù–ö–¶–ò–Ø
#     with open(path, "w", encoding="utf-8") as f:
#         f.write(text)

# --- –ù–û–í–ê–Ø –§–£–ù–ö–¶–ò–Ø ---
def load_known_ids():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ ID —Ç–≤–∏—Ç–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞."""
    try:
        with open(KNOWN_IDS_FILE, "r", encoding="utf-8") as f:
            return set(line.strip() for line in f if line.strip())
    except FileNotFoundError:
        return set()

def save_known_ids(ids):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—Å–µ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ ID —Ç–≤–∏—Ç–æ–≤ –≤ —Ñ–∞–π–ª."""
    with open(KNOWN_IDS_FILE, "w", encoding="utf-8") as f:
        for tweet_id in sorted(ids):
            f.write(tweet_id + "\n")
# --- –ö–û–ù–ï–¶ –ù–û–í–û–ô –§–£–ù–ö–¶–ò–ò ---

def fetch_tweets(cursor=None, limit=50):
    params = {"type": "Latest", "limit": limit}
    if cursor:
        params["cursor"] = cursor
    r = requests.get(BASE_URL, headers=HEADERS, params=params)
    r.raise_for_status()
    return r.json()


def collect_all_tweets():
    all_tweets = []  # –î–ª—è all_tweets.json (–Ω–æ–≤—ã–µ —Ç–≤–∏—Ç—ã –∑–∞ –∑–∞–ø—É—Å–∫)
    seen_ids_current_run = set() # –î–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –≤ –≠–¢–û–ú –∑–∞–ø—É—Å–∫–µ
    known_ids = load_known_ids() # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é ID
    cursor = None
    total_new = 0
    max_new_tweets = 2500  # –õ–∏–º–∏—Ç –Ω–∞ —Å–ª—É—á–∞–π, –µ—Å–ª–∏ API –Ω–µ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –≤–æ–æ–±—â–µ

    while True:
        data = fetch_tweets(cursor)
        tweets = data.get("tweets", [])
        cursor = data.get("next_cursor")

        if not tweets:
            logging.info("‚ùå –ù–µ—Ç –Ω–æ–≤—ã—Ö —Ç–≤–∏—Ç–æ–≤ –æ—Ç API.")
            break

        # --- –ö–õ–Æ–ß–ï–í–û–ï –ò–ó–ú–ï–ù–ï–ù–ò–ï: –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–æ—Ç–∏–≤ –í–°–ï–• –ò–°–¢–û–†–ò–ß–ï–°–ö–ò–• ID ---
        new_tweets = [t for t in tweets if t["id_str"] not in known_ids and t["id_str"] not in seen_ids_current_run]

        if not new_tweets:
            logging.info("‚úÖ –ù–æ–≤—ã—Ö —Ç–≤–∏—Ç–æ–≤ –±–æ–ª—å—à–µ –Ω–µ—Ç (–≤—Å–µ —Ç–≤–∏—Ç—ã —É–∂–µ –±—ã–ª–∏ –≤ –∏—Å—Ç–æ—Ä–∏–∏). –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–±–æ—Ä.")
            break

        all_tweets.extend(new_tweets)
        seen_ids_current_run.update(t["id_str"] for t in new_tweets)
        total_new += len(new_tweets)

        logging.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(new_tweets)} –Ω–æ–≤—ã—Ö —Ç–≤–∏—Ç–æ–≤ (–≤—Å–µ–≥–æ –Ω–æ–≤—ã—Ö –≤ —ç—Ç–æ–º –∑–∞–ø—É—Å–∫–µ: {len(all_tweets)})")

        # --- –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–´–ô –õ–ò–ú–ò–¢ ---
        if len(all_tweets) >= max_new_tweets:
            logging.warning(f"‚úÖ –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç –≤ {max_new_tweets} –Ω–æ–≤—ã—Ö —Ç–≤–∏—Ç–æ–≤. –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–±–æ—Ä.")
            break

        if not cursor:
            logging.info("‚úÖ –î–æ—Å—Ç–∏–≥–Ω—É—Ç –∫–æ–Ω–µ—Ü —Å–ø–∏—Å–∫–∞ —Ç–≤–∏—Ç–æ–≤ –æ—Ç API.")
            break

        time.sleep(3) # –£–≤–∞–∂–∞–µ–º –ª–∏–º–∏—Ç—ã API

    # --- –°–æ—Ö—Ä–∞–Ω—è–µ–º –¢–û–õ–¨–ö–û –ù–û–í–´–ï —Ç–≤–∏—Ç—ã –≤ all_tweets.json ---
    save_json(TWEETS_FILE, all_tweets)
    # --- –í–û–ó–í–†–ê–©–ê–ï–ú –°–û–ë–†–ê–ù–ù–´–ï –¢–í–ò–¢–´ –ò –û–ë–ù–û–í–õ–Å–ù–ù–´–ô –°–ü–ò–°–û–ö –ò–ó–í–ï–°–¢–ù–´–• ID ---
    final_known_ids = known_ids.copy() # –ö–æ–ø–∏—Ä—É–µ–º, —á—Ç–æ–±—ã –Ω–µ –∏–∑–º–µ–Ω—è—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª
    final_known_ids.update(t["id_str"] for t in all_tweets) # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–ø–∏—é
    logging.info(f"\n‚úÖ –°–±–æ—Ä –∑–∞–≤–µ—Ä—à—ë–Ω. –ù–æ–≤—ã—Ö —Ç–≤–∏—Ç–æ–≤: {len(all_tweets)}. –í—Å–µ–≥–æ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö ID –±—É–¥–µ—Ç: {len(final_known_ids)}")
    return all_tweets, final_known_ids # <-- –í–û–ó–í–†–ê–©–ê–ï–ú –û–ë–ê


def build_leaderboard(tweets):
    leaderboard = {}

    for t in tweets:
        user = t.get("user")
        if not user:
            continue
        name = user.get("screen_name")
        if not name:
            continue

        stats = leaderboard.setdefault(name, {
            "posts": 0,
            "likes": 0,
            "retweets": 0,
            "comments": 0,
            "quotes": 0,
            "views": 0
        })

        stats["posts"] += 1
        # --- –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –Ø–≤–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å None ---
        stats["likes"] += (t.get("favorite_count") or 0)
        stats["retweets"] += (t.get("retweet_count") or 0)
        stats["comments"] += (t.get("reply_count") or 0)
        stats["quotes"] += (t.get("quote_count") or 0)
        stats["views"] += (t.get("views_count") or 0)
        # --- –ö–û–ù–ï–¶ –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø ---

    leaderboard_list = [[user, stats] for user, stats in leaderboard.items()]
    save_json(LEADERBOARD_FILE, leaderboard_list)

    # --- –ö–û–î (–∞–≤—Ç–æ–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–∞—Ç—ã) –£–î–ê–õ–Å–ù ---
    # updated_at = datetime.now().strftime("%B %d, %Y")  # –ù–∞–ø—Ä–∏–º–µ—Ä: November 18, 2025
    # save_text(LAST_UPDATED_FILE, updated_at)
    # -----------------

    logging.info(f"üèÜ –õ–∏–¥–µ—Ä–±–æ—Ä–¥ –æ–±–Ω–æ–≤–ª—ë–Ω ({len(leaderboard_list)} —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤).")


if __name__ == "__main__":
    tweets, final_known_ids = collect_all_tweets() # <-- –ü–û–õ–£–ß–ê–ï–ú –ò–ó–í–ï–°–¢–ù–´–ï ID
    build_leaderboard(tweets) # <-- –°–ù–ê–ß–ê–õ–ê –ü–û–°–¢–†–û–ò–¢–¨
    save_known_ids(final_known_ids) # <-- –ü–û–¢–û–ú –°–û–•–†–ê–ù–ò–¢–¨ ID (–¢–û–õ–¨–ö–û –ï–°–õ–ò –í–°–Å –£–°–ü–ï–®–ù–û)
